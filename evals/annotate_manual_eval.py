"""
LLM-assisted helper to annotate manual eval outputs.

Usage:
  python -m evals.annotate_manual_eval

What it does:
1) Reads evals/manual_eval_log.csv (generated by run_manual_eval.py).
2) Uses an OpenAI model to propose issues_gaps and follow_ups for each row.
3) Writes evals/manual_eval_log_evaluated.csv with those annotations and blank
   reference_answer, is_correct, judge_notes columns.
"""

from __future__ import annotations

import csv
import json
import time
from pathlib import Path
from typing import Dict, Any, List
import os

from openai import OpenAI, RateLimitError

client = OpenAI()  # uses OPENAI_API_KEY from env

BASE_DIR = Path(__file__).resolve().parent
INPUT_PATH = BASE_DIR / "manual_eval_log.csv"
EVALUATED_PATH = BASE_DIR / "manual_eval_log_evaluated.csv"
WITH_BLANK_GT_PATH = None  # Deprecated; keep for backward compatibility signature


SYSTEM_PROMPT = """
You are annotating outputs from a water-level agent.

For each row you receive:
- question: the user's request
- answer: the agent's response
- tool_calls_observed: notes on which tools ran (if any)

Your job:
1) issues_gaps: Briefly note any correctness issues, missing context, or user risks.
   Focus on data validity (wrong/absent station, missing timestamps, hallucinated time windows, zero-only series).
2) follow_ups: Suggest the next 1-2 actions to fix or validate (e.g., try another station, ask for a shorter window, re-run with timestamps).

Return JSON with exactly:
{
  "issues_gaps": "...",
  "follow_ups": "..."
}
Keep both fields concise (1-2 sentences each). Use "none" if truly nothing to flag.
"""


def build_messages(row: Dict[str, str]) -> List[Dict[str, Any]]:
    question = row.get("question", "")
    answer = row.get("answer", "")
    tools = row.get("tool_calls_observed", "")
    user_content = f"""
Question:
{question}

Agent answer:
{answer}

Tool calls observed:
{tools}
"""
    return [
        {"role": "system", "content": SYSTEM_PROMPT.strip()},
        {"role": "user", "content": user_content.strip()},
    ]


def call_llm(messages, max_retries: int = 5):
    delay = 5
    for attempt in range(1, max_retries + 1):
        try:
            return client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4.1-mini"),
                messages=messages,
                response_format={"type": "json_object"},
            )
        except RateLimitError as e:
            if attempt == max_retries:
                raise
            print(
                f"Rate limit hit (attempt {attempt}/{max_retries}). "
                f"Sleeping {delay} seconds then retrying..."
            )
            time.sleep(delay)
            delay = min(delay * 2, 60)


def annotate_row(row: Dict[str, str]) -> Dict[str, str]:
    # If already annotated, keep existing values
    if row.get("issues_gaps") or row.get("follow_ups"):
        return row

    messages = build_messages(row)
    response = call_llm(messages)
    content = response.choices[0].message.content
    try:
        parsed = json.loads(content)
    except json.JSONDecodeError:
        parsed = {
            "issues_gaps": content or "",
            "follow_ups": "",
        }

    row["issues_gaps"] = (parsed.get("issues_gaps") or "").strip()
    row["follow_ups"] = (parsed.get("follow_ups") or "").strip()
    return row


def main():
    if not INPUT_PATH.exists():
        print(f"Input file not found: {INPUT_PATH}")
        return

    with INPUT_PATH.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        rows = list(reader)
        fieldnames = reader.fieldnames or []

    # Ensure expected columns exist
    required_cols = ["id", "category", "question", "answer", "tool_calls_observed", "issues_gaps", "follow_ups"]
    missing = [c for c in required_cols if c not in fieldnames]
    if missing:
        print(f"Missing expected columns in input: {missing}")
        return

    annotated_rows = []
    total = len(rows)
    for idx, row in enumerate(rows, start=1):
        print(f"Annotating row {idx}/{total} (id={row.get('id','')})...")
        try:
            annotated = annotate_row(row)
        except RateLimitError as e:
            print(f"RateLimitError encountered on row {idx}: {e}")
            raise
        annotated_rows.append(annotated)

    # Write evaluated file (same columns as input)
    gt_fieldnames = fieldnames.copy()
    for extra in ["reference_answer", "is_correct", "judge_notes"]:
        if extra not in gt_fieldnames:
            gt_fieldnames.append(extra)

    with EVALUATED_PATH.open("w", newline="", encoding="utf-8") as f_out:
        writer = csv.DictWriter(f_out, fieldnames=gt_fieldnames)
        writer.writeheader()
        for row in annotated_rows:
            new_row = row.copy()
            for extra in ["reference_answer", "is_correct", "judge_notes"]:
                if extra not in new_row:
                    new_row[extra] = ""
            writer.writerow(new_row)
    print(f"\nSaved annotated rows (with blank GT columns) to {EVALUATED_PATH}")


if __name__ == "__main__":
    main()
